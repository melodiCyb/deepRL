{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "In this project, an agent lies in a square world and aims to collect yellow bananas and avoid blue bananas via adjusting its navigation. State space consist to the agent's velocity and ray-based perception of objects around the agent's forward direction. Our task is episodic and using the state space information together with the rewards given below, the agent trained to choose best actions to achieve an average score of +13 over 100 consecutive episodes.\n",
    "\n",
    "* Reward of yellow banana: +1 \n",
    "* Reward of blue banana: -1\n",
    "* Dimension of the state space: 37\n",
    "\n",
    "Available actions (discrete): \n",
    "\n",
    "* 0 - move forward.\n",
    "* 1 - move backward.\n",
    "* 2 - turn left.\n",
    "* 3 - turn right.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Deep Q-learning to approximate Q-function. We design 2-layered DNN with Adam Optimizer: \n",
    "* First layer (37, 64)\n",
    "* Second layer (64, 4)\n",
    "\n",
    "where input dimension is the dimension of the state space: 37 and output dimension at the last layer is the dimension of action vector: 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve performance of Q-learning we used Experience Replay and Fixed Q-value methods. In the Experience Replay we use a buffer memory to store experiences and then randomly sample from these experiences to use in learning step multiple times. Randomized selection helps to avoid harmful correlations in the sequential experiences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Fixed Q-value method we aim to avoid instability caused by the dependency of TD target on the weights w and we use a separate network as target network with identical architecture. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "    BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "    BATCH_SIZE = 64         # minibatch size\n",
    "    GAMMA = 0.99            # discount factor\n",
    "    TAU = 1e-3              # for soft update of target parameters\n",
    "    LR = 5e-4               # learning rate \n",
    "    UPDATE_EVERY = 4        # how often to update the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    n_episodes=1800         # maximum number of training episodes\n",
    "    max_t=1000              # maximum number of timesteps per episode\n",
    "    eps_start=1.0           # starting value of epsilon, for epsilon-greedy action selection\n",
    "    eps_end=0.01            # minimum value of epsilon\n",
    "    eps_decay=0.995         # multiplicative factor (per episode) for decreasing epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "![p1_score](p1_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average score of 13.06 in the last 100  episodes was achieved in 515 episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "As a future work, I would like to experiment variations of DQN e.g double DQN and dueling DQN and prioritized experience replay to improve agent's performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
