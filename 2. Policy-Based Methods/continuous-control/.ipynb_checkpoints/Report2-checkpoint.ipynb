{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "In this project we are given an environment of a double-jointed arm that can perform reaching tasks. In our task we give reward of 0.1 for each time step that the arm is in the goal location and try to maintain this position as long as possible to achieve the highest cumulative reward.\n",
    "\n",
    "\n",
    "* Dimension of the observation spacee: 33 \n",
    "* variables: position, rotation, velocity, and angular velocities of the arm. \n",
    "* Each action is a vector with four numbers, corresponding to torque applicable to two joints. \n",
    "* Every entry in the action vector should be a number between -1 and 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "As a learning algorithm we used Deep Deterministic Policy Gradient (DDPG), an actor-critic method. DDPG consists to two neural networks, one for the actor and the other for the critic. These networks calculate action vectors for the current state and generate a temporal-difference error signal each time step.The critic network is updated from the gradients obtained from the TD error signal. The estimated Q-value of the current state is obtained from the critic network's output.\n",
    "\n",
    "\n",
    "* We used 3 fully connected layers with dimensions of (state_size x 400), (400, 300) and (300, action_size)\n",
    "* In order to stabilize learning we used gradient clipping and batch normalization.\n",
    "* Activation function leaky relu with leakiness parameter of 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "        BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "        BATCH_SIZE = 128        # minibatch size\n",
    "        GAMMA = 0.99            # discount factor\n",
    "        TAU = 1e-3              # for soft update of target parameters\n",
    "        LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "        LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "        EPSILON_DECAY = 1e-6    # noise epsilon decay \n",
    "        PERIOD = 20             # learning interval\n",
    "        NUM_LEARN = 10         # number of learning passes\n",
    "        \n",
    " \n",
    "        \n",
    "        n_episodes = 3000  # maximum number of training episodes\n",
    "        max_t = 5000       # maximum number of timesteps per episode\n",
    "        \n",
    "        Ornstein-Uhlenbeck noise parameters\n",
    "        \n",
    "        mu = 0.0\n",
    "        theta = 0.15\n",
    "        sigma = 0.2\n",
    " \n",
    "In the experiments using 1e-4 lr for actor slowed down the training a lot and it got very fast by changing it to 1e-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![p2_scores](p2_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "* I would like to experiment with different approaches such as PPO and D4PG and second environment versino with 20 agents.\n",
    "* I would like to practice in different environments with similar tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
