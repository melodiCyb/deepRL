{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "As a learning algorithm Multi-Agent Deep Deterministic Policy Gradient (MADDPG) is used. DDPG consists to two neural networks for actor and critic. There are two agents in this environment that compete to each other. thus two differentInput for the actor network is the current state and output is the action. These networks calculate action vectors for the current state and and generate a temporal-difference error signal each time step.The critic network is updated from the gradients obtained from the TD error signal. The estimated Q-value of the current state is obtained from the critic network's output. \n",
    "\n",
    "\n",
    "* state_size = 24  # Dimension of each state\n",
    "* action_size = 2 # Dimension of each action \n",
    "* fcs1_units = 128  # Number of nodes in the first hidden layer\n",
    "* fc2_units = 128 # Number of nodes in the second hidden layer\n",
    " \n",
    " We also used 1d batch normalization to stabilize the training and reduce the number of required training steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "    BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "    BATCH_SIZE = 128        # minibatch size\n",
    "    GAMMA = 0.99            # discount factor\n",
    "    TAU = 1e-3              # for soft update of target parameters\n",
    "    LR_ACTOR = 2e-4         # learning rate of the actor \n",
    "    LR_CRITIC = 2e-4        # learning rate of the critic\n",
    "    WEIGHT_DECAY = 0        # L2 weight decay\n",
    "    \n",
    "    Noise params in Ornstein-Uhlenbeck process\n",
    "    mu=0.0\n",
    "    theta=0.15\n",
    "    sigma=0.1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![p3_scores](p3_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment solved in 1428 episodes!\tAverage Score: 0.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Experiment using different algorithms such as D4PG, PP0 or A3C.\n",
    "* Improve hyperparameter settings and try different network architectures\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
